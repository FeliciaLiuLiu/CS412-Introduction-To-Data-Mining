Week 6
Class Questions:

1. Do clustering algorithms require labels for the input data instances?
Ans: No
Exp: No. Clustering is unsupervised learning; it does not require labels for the input instances.

2. Can we use clustering to help with classification tasks?
Ans: Yes
Exp: Yes. We can use clustering as a preprocessing step in classification tasks to help us deal with feature sparsity issues as well as engineering complex features. 
For example, in a medical database, each patient may have a distinct real-valued measure for certain tests (e.g., glucose, cholesterol). 
Clustering patients first may help us understand how binning should be done on real-valued features to reduce feature sparsity and improve accuracy on classification tasks such as survival prediction.

3. In which of the following scenarios is hierarchical partitioning likely to be desired over single level partitioning? Select all that apply.
Ans:
(1) Clustering organisms based on their characteristics (e.g., appearance, behaviors, genetic makeup) (Hint: think about taxonomy in biology.)
(2) Clustering documents by topics, where there might be inherent hierarchy in the topics (e.g., Computer Science -> Data Mining -> Cluster Analysis)
(3) Clustering people by geolocation where we may wish to examine the population at different levels of resolution (e.g., Country->State->Municipality)
Exp:
All three choices. These are all examples where hierarchical partitioning is helpful for examining the data at different levels of resolution.

4. From the following data types, choose the ones where cluster analysis can be applied. Select all that apply.
Ans: Graph data; Text documents; Time series
Exp:
All three choices. These are all example data types where cluster analysis can be applied. 

5. Which of the following are partition-based clustering algorithms? Select all that apply.
Ans: K-Means; K-Medoids

6. Which of the following data are categorical? Select all that apply.
Ans: Sex; Race

7. Consider a heterogeneous network of publications that contains nodes of type author, paper, and venue, where papers connect to authors via the written-by relation, papers connect to venues via the published-in relation, and papers connect to each other via the cited-by relation. 
What are some feasible ways to transform this network into a homogeneous network?  
Ans:
(1) We can create a co-authorship network by keeping authors and creating links between authors that are connected to the same paper.  
(2) We can create a citation network by only keeping papers and the cited-by links (induced subgraph on nodes of the type paper).  
Exp:
Both. Here we give two examples of projecting a heterogeneous network into a homogeneous network by leaving out certain information about the complete dataset for the sake of homogeneity. 

8. Which of the following statements is true?
Ans: 
A good clustering algorithm should achieve high similarity between the data points within the same cluster.   
Exp:
A good clustering algorithm should achieve high similarity between the data points within the same cluster.    

9. Given the two-dimensional points (0, 3) and (4, 0), what is the Manhattan distance between those two points?
Ans: 7
Exp: 7. The Manhanttan distance is |0 - 4| + |3 - 0| = 7.

10. Given three vectors A, B, and C, suppose the cosine
similarity between A and B is cos(A, B) = 1.0, and the similarity between A and
C is cos(A, C) = -1.0.  Can we determine
the cosine similarity between B and C?
Ans: Yes
Exp: Yes. The consine similiarity between B and C is -1.0.

11. Suppose X is a random variable with P(X = -1) = 0.5 and P(X = 1) = 0.5.
In addition, we have another random variable Y=X * X.
What is the covariance between X and Y?
Ans: 0 
Exp:
E[X] = 0 and E[Y] = 1.  E[XY] = 0.5*(-1*1) + 0.5 * 1 * 1 = 0. Thus, covariance(X, Y) = E(XY) – E(X) * E(Y) = 0. Obviously, X and Y are dependent. However, covariance between X and Y is 0.   


Quiz:

1. Which of the following statements are true? Select all that apply.
Ans:
(1) Clustering analysis has a wide range of applications in tasks such as data summarization, dynamic trend detection, multimedia analysis, and biological network analysis.
(2) Clustering analysis is unsupervised learning since it does not require labeled training data.

2. What are some common considerations and requirements for cluster analysis? Select all that apply.
Ans:
(1) We need to consider the space on which the clustering is performed. In other words, we need to decide what subset of the available features we are going to consider in the similarity measure.  
(2) We need to consider the desired shape and size of clusters.
(3) In order to perform cluster analysis, we need to have a similarity measure between data objects.

3. Which of the following statements are true? Select all that apply.
Ans:
(1) K-means is an example of a distance-based clustering method.
(2) Dimensionality reduction helps make high-dimensional clustering more feasible and scalable.

False:
(1) Since cluster analysis is unsupervised learning, there’s no way to incorporate user preference or guidance into the clustering process.
(2) There are no clustering algorithms that can handle time-series data since we always assume that the data points are temporally independent from each other.

4. If you need to choose between clustering and supervised learning for the following applications, for which ones would you choose clustering over supervised learning? Select all that apply.
Ans:
(1) Find user communities in online social networks such as Facebook and Twitter.
(2) Given a large number of web pages, discover the latent topics discussed by those web pages.

False:
(1) A hospital has collected the health measurements for a number of healthy people and the patients who are infected with disease X; it needs to determine
whether a new person is infected with disease X after collecting his/her health.
(2) Given a large number of emails, we already know whether each email is spam or not. Now we need to determine whether a newly incoming email is spam.

data.
5. Which of the following statements are true? Select all that apply.
Ans:
(1) When clustering, we want to put two data objects that are similar into the same cluster.
(2) Clustering analysis has a wide range of applications in tasks such as data summarization, dynamic trend detection, multimedia analysis, and biological network analysis.

False:
(1) Cluster analysis is considered supervised learning.
(2) It is impossible to cluster objects in a data stream. We must have all the data objects that we need to cluster ready before clustering can be performed.

6. What are some common considerations and requirements for cluster analysis? Select all that apply.
Ans:
(1) In order to perform cluster analysis, we need to have a similarity measure between data objects.  
(2) We need to be able to handle a mixture of different types of attributes (e.g., numerical and categorical).
(3) We need to consider how to incorporate user preference for cluster size and shape into the clustering algorithm. 

False:
(1) Cluster analysis requires a large amount of labeled training data.  

7. Which of the following statements are true? Select all that apply.
Ans:
(1) Agglomerative clustering is an example of a distance-based clustering method.
(2) Graphs, time-series data, text, and multimedia data are all examples of data types on which cluster analysis can be performed.

False:


8. Which of the following statements are true? Select all that apply.
Ans:
(1) When clustering, we want to put two data objects that are similar into the same cluster.
(2) Clustering analysis has a wide range of applications in tasks such as data summarization, dynamic trend detection, multimedia analysis, and biological network analysis.

Incorrect:
(1) It is impossible to cluster objects in a data stream. We must have all the data objects that we need to cluster ready before clustering can be performed.
(2) Cluster analysis is considered supervised learning.

9. The following real dataset contains information about two different flowers: Iris setosa and Iris versicolor. 

Species             Sepal length    Sepal width     Petal length      Petal width

Iris setosa           4.9              3.0              1.4               0.2

Iris versicolor       5.6              2.5              3.9               1.1

What is the Manhattan distance between these two objects?

Ans: 4.6

Exp: 
The Manhattan distance, corresponding to p = 1 Minkowski distance, between two objects is defined as follows:



What is the supremum distance between these two objects? 

Ans: 2.5
Exp:
The supremum distance, corresponding to p = infinity Minkowski distance, between two objects is defined as follows: 



What is the Euclidean distance between these two objects? 
Ans: 2.8
Exp:
The Euclidean distance between two objects is defined as follows:


10. The following real dataset contains two samples from the dataset for Prediction of Molecular Bioactivity for Drug Design – Binding to Thrombin, with sampled features. For each activity (F1, F2, …, F10), the class value (0/1) indicates if the activity is active or inactive. 

Case  F1  F2  F3  F4  F5  F6  F7  F8  F9  F10
  1   0   1   1   0   0   0   1   1   1    1
  2   0   1   0   0   1   0   1   0   0    1

Assume all the activities are symmetric binary variables. What is the distance between case 1 and case 2? 

Ans: 4/10
Exp:
We have the contingency table as follows:

If the binary variables are symmetric, we have: 

d(i,j) = (r+s) / ( q+r+s+t ) = 4/10



Assume all the activities are asymmetric binary variables. What is the distance between case 1 and case 2?

Ans: 4/7
Exp:
We have the contingency table as follows:

If the binary variables are asymmetric, we have:
d(i,j) = (r+s) / (q+r+s) = 4/7






11. The following real world dataset contains two samples from Car Evaluation Database, 
which was derived from a simple hierarchical decision model originally developed for the demonstration of DEX (Bohanec, M., & Rajkovic, V. (1990). 
Expert system for decision making. Sistemica 1(1), 145-157). The model evaluates cars according to the following concept structure:

Case    buying    maint    doors    persons     lug_boot    safety
Car 1   med       v-high    3         more        small       med
Car 2   high      v-high    4           4          big        med


Assume we use simple match to estimate the proximity between objects. What is the distance between Car 1 and Car 2?

Ans: 2/3
Exp:
Considering simple match, there are 2 out of 6 features of the two cars that are the same, thus the distance is:
d(i,j) = (p-m) / p = 6-2 / 6 = 2/3


To calculate the distance between objects with categorical attributes, we use a set of binary attributes to represent each categorical attribute. 
Assume all the binary attributes are asymmetric. What is the distance between Car 1 and Car 2?     





To calculate the distance between objects with categorical attributes, we use a set of binary attributes to represent each categorical attribute. 
Assume all the binary attributes are symmetric. What is the distance between Car 1 and Car 2? 

Ans: 8/21
Exp:
Considering converting the categorical random variables into binary random variables, 
there will be (4 + 4 + 4 + 3 + 3 + 3 = 21) binary random variables in total. Moreover, we have the following contingency table.

If all the binary attributes are asymmetric, we have the distance between Car 1 and Car 2 as

d(i,j) = (s+r) / (q+s+r+t) = 8/21

12. Consider a two-dimensional space. Given a query point Q = (0.8, 0.6), which of the following is the closest to Q in terms of cosine similarity?
Ans: (16, 12)
Exp:
(16, 12) is the closest. The two points (0.8, 0.6) and (16, 12) are in exactly the same direction; 
their cosine similarity is 1.0, which is the largest among the given points.


Ans: (32, 24)
Exp:
(32, 24) is the closest. The two points (0.8, 0.6) and (32, 24) are in exactly the same direction; 
their cosine similarity is 1.0, which is the largest among the given points.


Consider a two-dimensional space. Given a query point Q = (0.8, 0.6), which of the following is the farthest to Q in terms of cosine similarity?



13. Given the following two short texts with punctuation removed, calculate the cosine similarity between them based on the bag of words model.  
Text1: language is the source of misunderstandings  
Text2:  language is the soul of a nation

Ans: 0.617
Exp:
We can get the vector representations for the two short texts,  
T1 = (1, 1, 1, 1, 1, 1, 0, 0, 0) 
T2 = (1, 1, 1, 0, 1, 0, 1, 1, 1), 
where the dimensions correspond to language, is, the, source, of, misunderstanding, soul, a, nation. 
The cosine similarity can be obtained via
cos(T1, T2) = T1*T2 / |T1|*|T2| = 4 / √6√7 = 0.617


14. Given the following two short texts with punctuation removed, calculate the cosine similarity between them based on the bag of words model.  
Text1: all grown-ups were once children but only few of them remember it 
Text2: all children should be very understanding of grown-ups

Ans: 0.408
Exp:
We can get the vector representations for the two short texts,  
T1 = (1, 1, 1, 1, 1, 1, 1, 1, 1, 1,1, 1, 0, 0, 0, 0) 
T2 = (1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1), 
where the dimensions correspond to all, grown-ups, were, once, children, but, only, few, of, them, remember, it, should, be, very, understanding. 
The cosine similarity can be obtained via
cos(T1, T2) = T1*T2 / |T1|*|T2| = 4 / √12√8 = 0.408



15. Given the following two short texts with punctuation removed, calculate the cosine similarity between them based on the bag of words model. 
Text1: one sees clearly only with the heart anything essential is invisible to the eyes 
Text2:  let my soul smile through my heart and my heart smile through my eyes that I may scatter rich smiles in sad hearts

Ans: 0.117
Exp:
We can get the vector representations for the two short texts,  
T1 = (1 1 1 1 1 1 1 1 1 2 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) 
T2 = (2 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 2 1 2 4 1),  
where the dimensions correspond to: heart, eyes, sees, anything, is, one, to, only, invisible, the, with, clearly, essential, and, hearts, that, I, in, soul, sad, may, let, rich, smile, smiles, through, my, scatter. 
The cosine similarity can be obtained via
cos(T1, T2) = T1*T2 / |T1|*|T2| = 3 / √16√41 = 0.117



16. Question 6
With regard to the species of Iris setosa, we have sampled data on the features of sepal length and sepal width, as follows. 

Feature   Sepal length    Sepal width
Case 1        5.1             3.5
Case 2        4.9             3.0
Case 3        4.7             3.2
Case 4        4.6             3.1
Case 5        5.0             5.4

What is the sample correlation coefficient between sepal length and sepal width? 


Ans: 0.479

Exp:
The sample correlation coefficient can be calculated as:
